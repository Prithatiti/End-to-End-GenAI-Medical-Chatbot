{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'e:\\\\End to End Medical Chatbot\\\\Medical-Chatbot\\\\research'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# To know the current file path\n",
    "\n",
    "%pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'e:\\\\End to End Medical Chatbot\\\\Medical-Chatbot'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Make sure that we are in the root directory\n",
    "\n",
    "import os\n",
    "os.chdir(\"../\")\n",
    "\n",
    "%pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Pritha\\anaconda3\\envs\\medibot\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from langchain_community.document_loaders import PyPDFLoader, DirectoryLoader\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain_community.embeddings import HuggingFaceEmbeddings\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from huggingface_hub import hf_hub_download"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract the Data from The PDF file\n",
    "\n",
    "def load_pdf_file(data):\n",
    "    loader = DirectoryLoader(\n",
    "        data,\n",
    "        glob = \"*.pdf\",\n",
    "        loader_cls = PyPDFLoader\n",
    "    )\n",
    "\n",
    "    documents = loader.load()\n",
    "\n",
    "    return documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "extracted_data = load_pdf_file(data = 'Data/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#extracted_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the Data into Text Chunks\n",
    "\n",
    "def text_spliter(extracted_data):\n",
    "    text_splitter = RecursiveCharacterTextSplitter(\n",
    "        chunk_size = 500,\n",
    "        chunk_overlap = 20\n",
    "    )\n",
    "\n",
    "    text_chunks = text_splitter.split_documents(extracted_data)\n",
    "\n",
    "    return text_chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Length of the Text Chunks: 39994\n"
     ]
    }
   ],
   "source": [
    "text_chunks = text_spliter(extracted_data)\n",
    "print(f\"Length of the Text Chunks: {len(text_chunks)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# text_chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download the Embeddings from Hugging Face\n",
    "\n",
    "def download_hugging_face_embedding():\n",
    "    embeddings = HuggingFaceEmbeddings(\n",
    "        model_name = \"sentence-transformers/all-MiniLM-L6-v2\"\n",
    "    )\n",
    "\n",
    "    return embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Pritha\\AppData\\Local\\Temp\\ipykernel_13112\\1833081923.py:4: LangChainDeprecationWarning: The class `HuggingFaceEmbeddings` was deprecated in LangChain 0.2.2 and will be removed in 1.0. An updated version of the class exists in the :class:`~langchain-huggingface package and should be used instead. To use it run `pip install -U :class:`~langchain-huggingface` and import as `from :class:`~langchain_huggingface import HuggingFaceEmbeddings``.\n",
      "  embeddings = HuggingFaceEmbeddings(\n"
     ]
    }
   ],
   "source": [
    "embeddings = download_hugging_face_embedding()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Length: 384\n"
     ]
    }
   ],
   "source": [
    "query_result = embeddings.embed_query(\"Hello World\")\n",
    "print(f\"Length: {len(query_result)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# query_result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "PINECONE_API_KEY = os.environ.get(\"PINECONE_API_KEY\")\n",
    "GOOGLE_API_KEY = os.getenv(\"GOOGLE_API_KEY\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Pritha\\anaconda3\\envs\\medibot\\Lib\\site-packages\\pinecone\\data\\index.py:1: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from tqdm.autonotebook import tqdm\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'Pinecone' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[1], line 5\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mpinecone\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m pinecone, ServerlessSpec\n\u001b[0;32m      4\u001b[0m \u001b[38;5;66;03m# Create a Pinecone client instance\u001b[39;00m\n\u001b[1;32m----> 5\u001b[0m pc \u001b[38;5;241m=\u001b[39m Pinecone(api_key \u001b[38;5;241m=\u001b[39m PINECONE_API_KEY)\n\u001b[0;32m      7\u001b[0m index_name \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124maimedibot\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m      9\u001b[0m \u001b[38;5;66;03m# Create an index\u001b[39;00m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'Pinecone' is not defined"
     ]
    }
   ],
   "source": [
    "from langchain_pinecone import PineconeVectorStore\n",
    "from pinecone import Pinecone, ServerlessSpec\n",
    "\n",
    "# Create a Pinecone client instance\n",
    "pc = Pinecone(api_key = PINECONE_API_KEY)\n",
    "\n",
    "index_name = \"aimedibot\"\n",
    "\n",
    "# Create an index\n",
    "pc.create_index(\n",
    "    name = index_name,\n",
    "    dimension = 384,  # Replace with your model's dimensions\n",
    "    metric = \"cosine\",  # Replace with your preferred metric\n",
    "\n",
    "    spec = ServerlessSpec(\n",
    "        cloud = \"aws\",\n",
    "        region = \"us-east-1\"\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"PINECONE_API_KEY\"] = PINECONE_API_KEY\n",
    "os.environ[\"GOOGLE_API_KEY\"] = GOOGLE_API_KEY"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'index_name' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[19], line 7\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# Embed each chunk and upsert the embeddings into your pinecone index\u001b[39;00m\n\u001b[0;32m      3\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mlangchain_pinecone\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m PineconeVectorStore\n\u001b[0;32m      5\u001b[0m docSearch \u001b[38;5;241m=\u001b[39m PineconeVectorStore\u001b[38;5;241m.\u001b[39mfrom_documents(\n\u001b[0;32m      6\u001b[0m     documents \u001b[38;5;241m=\u001b[39m text_chunks,\n\u001b[1;32m----> 7\u001b[0m     index_name \u001b[38;5;241m=\u001b[39m index_name,\n\u001b[0;32m      8\u001b[0m     embedding \u001b[38;5;241m=\u001b[39m embeddings\n\u001b[0;32m      9\u001b[0m )\n",
      "\u001b[1;31mNameError\u001b[0m: name 'index_name' is not defined"
     ]
    }
   ],
   "source": [
    "# Embed each chunk and upsert the embeddings into your pinecone index\n",
    "\n",
    "from langchain_pinecone import PineconeVectorStore\n",
    "\n",
    "docSearch = PineconeVectorStore.from_documents(\n",
    "    documents = text_chunks,\n",
    "    index_name = index_name,\n",
    "    embedding = embeddings\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Connect to the existing index for document search\n",
    "\n",
    "docSearch = PineconeVectorStore.from_existing_index(\n",
    "    index_name = index_name,  # Use the created index\n",
    "    embedding = embeddings  # Embedding model used for vectorizing queries/documents\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<langchain_pinecone.vectorstores.PineconeVectorStore at 0x235de5d2b00>"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "docSearch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "retriver = docSearch.as_retriever(  # Convert the Pinecone vector store into a retriever\n",
    "    search_type = \"similarity\",       # Use similarity-based search to find the closest matches\n",
    "    search_kwargs = {'k': 3}          # Retrieve the top 3 most similar results\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "retrived_docs = retriver.invoke(\"What is breast cancer?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(id='96207d27-e8cc-4304-8541-07ea498bcf94', metadata={'creationdate': '2006-10-16T20:19:33+02:00', 'creator': 'Adobe Acrobat 6.0', 'moddate': '2006-10-16T22:03:45+02:00', 'page': 671.0, 'page_label': '642', 'producer': 'PDFlib+PDI 6.0.3 (SunOS)', 'source': 'Data\\\\The Gale Encyclopedia of Medicine (3rd Edition).pdf', 'total_pages': 4505.0}, page_content='Definition\\nBreast cancer is caused by the development of\\nmalignant cells in the breast. The malignant cells\\noriginate in the lining of the milk glands or ducts of\\nthe breast (ductal epithelium), defining this malig-\\nnancy as a cancer. Cancer cells are characterized by\\nuncontrolled division leading to abnormal growth\\nand the ability of these cells to invade normal tissue\\nlocally or to spread throughout the body, in a process\\ncalled metastasis.\\nDescription'),\n",
       " Document(id='b314c338-6164-44e3-92e2-28ac3db45a5d', metadata={'creationdate': '2006-10-16T20:19:33+02:00', 'creator': 'Adobe Acrobat 6.0', 'moddate': '2006-10-16T22:03:45+02:00', 'page': 671.0, 'page_label': '642', 'producer': 'PDFlib+PDI 6.0.3 (SunOS)', 'source': 'Data\\\\The Gale Encyclopedia of Medicine (3rd Edition).pdf', 'total_pages': 4505.0}, page_content='Description\\nBreast cancer arises in the milk-producing\\nglands of the breast tissue. Groups of glands in\\nnormal breast tissue are called lobules. The products\\nof these glands are secreted into a ductal system that\\nleads to the nipple. Depending on where in the\\nglandular or ductal unit of the breast the cancer\\narises, it will develop certain characteristics that\\nare used to sub-classify breast cancer into types.\\nThe pathologist will denote the subtype at the time of'),\n",
       " Document(id='2326ae0e-bea6-49f1-b964-01848667d7ff', metadata={'creationdate': '2006-10-16T20:19:33+02:00', 'creator': 'Adobe Acrobat 6.0', 'moddate': '2006-10-16T22:03:45+02:00', 'page': 672.0, 'page_label': '643', 'producer': 'PDFlib+PDI 6.0.3 (SunOS)', 'source': 'Data\\\\The Gale Encyclopedia of Medicine (3rd Edition).pdf', 'total_pages': 4505.0}, page_content='A breast cancer cell. (Phototake NYC. Reproduced by\\npermission.)\\nMammogram indicating a tumor in the center of the breast.\\n(Chris Bjornberg, Photo Researchers. Reproduced by\\npermission.)\\nGALE ENCYCLOPEDIA OF MEDICINE 643\\nBreast cancer')]"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "retrived_docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_google_genai import ChatGoogleGenerativeAI\n",
    "\n",
    "\n",
    "# Initialize the Gemini model\n",
    "llm = ChatGoogleGenerativeAI(\n",
    "    model = \"gemini-2.0-flash\", \n",
    "    google_api_key = GOOGLE_API_KEY\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "content=\"Gemini is a family of **large language models (LLMs)** developed by Google AI. It's designed to be multimodal, meaning it can understand and generate text, images, audio, video, and code. Think of it as a very advanced AI that can process and create a wide range of content.\\n\\nHere's a breakdown of key aspects:\\n\\n*   **Multimodal Capabilities:** This is a core feature. Gemini isn't just about text like many other LLMs. It's designed to seamlessly integrate and reason across different types of information. For example, it can look at an image and write a description, or listen to audio and summarize the content.\\n\\n*   **Large Language Model (LLM):** Like other LLMs (such as GPT-4), Gemini is trained on a massive dataset of text and other data. This allows it to learn patterns, relationships, and nuances in language and other modalities, enabling it to generate human-quality content, translate languages, answer questions, and much more.\\n\\n*   **Different Versions:** Google has released different versions of Gemini tailored for specific purposes:\\n\\n    *   **Gemini Ultra:** The most powerful and capable model, intended for highly complex tasks. Initially available to select customers and developers for testing and feedback. Powers Google's AI Studio and Vertex AI.\\n    *   **Gemini Pro:** A more balanced model in terms of performance and resource requirements. It is being used to power Google's Bard (now Gemini) chatbot.\\n    *   **Gemini Nano:** A smaller model designed for on-device use, such as on smartphones.\\n\\n*   **Applications:** Gemini has a wide range of potential applications, including:\\n\\n    *   **Chatbots and Virtual Assistants:** Powering more intelligent and responsive conversational AI.\\n    *   **Content Creation:** Generating text, images, audio, and video for various purposes.\\n    *   **Code Generation:** Assisting programmers with writing and debugging code.\\n    *   **Research and Development:** Accelerating scientific discovery by analyzing and interpreting complex data.\\n    *   **Education:** Providing personalized learning experiences and tutoring.\\n\\n*   **Key Benefits Highlighted by Google:**\\n    *   **Reasoning:** Gemini is designed to handle complex reasoning tasks.\\n    *   **Coding:** Excels at understanding and generating code.\\n    *   **Multimodality:** Native understanding of different data types.\\n\\nIn summary, Gemini is Google's most advanced AI model, designed to be a versatile tool for understanding and generating a wide range of content, with the potential to revolutionize many industries.\" additional_kwargs={} response_metadata={'prompt_feedback': {'block_reason': 0, 'safety_ratings': []}, 'finish_reason': 'STOP', 'safety_ratings': []} id='run-4b7dcb3b-3de9-4a0e-935d-97af5e2ec9c6-0' usage_metadata={'input_tokens': 6, 'output_tokens': 538, 'total_tokens': 544, 'input_token_details': {'cache_read': 0}}\n"
     ]
    }
   ],
   "source": [
    "# Test the model\n",
    "response = llm.invoke(\"Hello! What is Gemini?\")\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary modules from LangChain\n",
    "from langchain.chains import create_tagging_chain  # For tagging and categorization tasks\n",
    "from langchain.chains.combine_documents import create_stuff_documents_chain  # Combines retrieved documents\n",
    "from langchain_core.prompts import ChatPromptTemplate  # Helps in structuring prompts\n",
    "from langchain.chains import create_retrieval_chain  # Creates a retrieval-based chain\n",
    "\n",
    "\n",
    "\n",
    "# Define the system prompt for the assistant\n",
    "system_prompt = (\n",
    "    \"You are an assistant for question-answering tasks.\"  # Specifies the AI's role\n",
    "    \"Use the following pieces of retrieved context to answer the question.\"  # Instructs AI to use provided context\n",
    "    \"If you don't know the answer, say that sorry, you don't know.\"  # Ensures AI doesn't hallucinate answers\n",
    "    \"Use three sentences maximum and keep the answer concise.\"  # Encourages short and precise responses\n",
    "    \"\\n\\n\"  # Adds a newline for better formatting\n",
    "    \"{context}\"  # Placeholder where the retrieved context will be inserted\n",
    ")\n",
    "\n",
    "\n",
    "\n",
    "# Create a structured prompt template using ChatPromptTemplate\n",
    "prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\"system\", system_prompt),  # The system message containing AI instructions\n",
    "        (\"human\", \"{input}\"),  # Placeholder for the user input/question\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creates a chain that takes the LLM (language model) and the prompt\n",
    "# This chain structures the retrieved documents into a clear, usable format for the LLM to generate an answer\n",
    "question_answer_chain = create_stuff_documents_chain(llm, prompt)\n",
    "\n",
    "\n",
    "# Creates a Retrieval-Augmented Generation (RAG) chain\n",
    "# This combines the retriever (which fetches relevant documents) with the question-answer chain\n",
    "# It ensures the LLM gets the right context before generating a response\n",
    "rag_chain = create_retrieval_chain(retriver, question_answer_chain)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Acne is a common skin disease characterized by pimples on the face, chest, and back. It occurs when the pores of the skin become clogged with oil, dead skin cells, and bacteria. Acne vulgaris is the medical term for common acne.\n"
     ]
    }
   ],
   "source": [
    "# Sends a query to the RAG (Retrieval-Augmented Generation) chain for answering\n",
    "response = rag_chain.invoke({\"input\": \"What is Acne?\"})\n",
    "\n",
    "# Extracts and prints the \"answer\" field from the response dictionary\n",
    "print(response[\"answer\"])\n",
    "\n",
    "# If the question is relevant to the data the retriever finds, \n",
    "# the model provides an accurate answer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The provided text discusses wilderness medicine, pathologists, and skin biopsies. It does not contain any information about geography. Therefore, I don't know the answer.\n"
     ]
    }
   ],
   "source": [
    "# Sends a user question (\"What is geography?\") into the RAG (Retrieval-Augmented Generation) chain\n",
    "response = rag_chain.invoke({\"input\": \"What is geography?\"})\n",
    "\n",
    "# Extracts and prints the \"answer\" field from the response dictionary\n",
    "print(response[\"answer\"])\n",
    "\n",
    "# If the retrieved context doesn't include relevant information about \n",
    "# \"geography,\" the model responds with a polite fallback message"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "medibot",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
